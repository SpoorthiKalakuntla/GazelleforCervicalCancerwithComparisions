{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4c501ab-d766-4808-b082-3e9ea72211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- 1. N/C Ratio Feature Extraction ---\n",
    "def extract_nc_ratio(image):\n",
    "    img_gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    _, nucleus_mask = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    cytoplasm_mask = cv2.bitwise_not(nucleus_mask)\n",
    "\n",
    "    nucleus_area = np.sum(nucleus_mask > 0)\n",
    "    cytoplasm_area = np.sum(cytoplasm_mask > 0)\n",
    "    \n",
    "    nc_ratio = nucleus_area / (cytoplasm_area + 1e-5)\n",
    "    return np.array([nc_ratio], dtype=np.float32)\n",
    "\n",
    "# --- 2. Custom Dataset ---\n",
    "class CervicalDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        nc_ratio = extract_nc_ratio(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(nc_ratio), label\n",
    "\n",
    "# --- 3. Preprocessing & Load Data ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image_paths_labels(data_dir):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "\n",
    "    for label in class_names:\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img in os.listdir(label_path):\n",
    "                paths.append(os.path.join(label_path, img))\n",
    "                labels.append(class_to_idx[label])\n",
    "\n",
    "    return paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08c9cefe-c1e8-40a5-b4dd-0b8e25aefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this to your dataset path\n",
    "data_dir = r'C:\\Users\\CVR\\Desktop\\DataSet\\Herlev Dataset\\augmented_train'\n",
    "image_paths, labels = load_image_paths_labels(data_dir)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_paths, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "train_dataset = CervicalDataset(X_train, y_train, transform)\n",
    "val_dataset = CervicalDataset(X_val, y_val, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e3962a21-4cb8-4922-b47d-6991eadb6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNetWithNC(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SqueezeNetWithNC, self).__init__()\n",
    "        self.squeezenet = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace final conv layer to match num_classes\n",
    "        self.squeezenet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        self.squeezenet.num_classes = num_classes\n",
    "        \n",
    "        # N/C feature processing (optional, if using handcrafted features)\n",
    "        self.nc_fc = nn.Linear(1, 32)\n",
    "        self.final_fc = nn.Linear(num_classes + 32, num_classes)\n",
    "\n",
    "    def forward(self, x, nc_ratio):\n",
    "        x = self.squeezenet.features(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        nc_feat = nn.functional.relu(self.nc_fc(nc_ratio.view(-1, 1)))\n",
    "        \n",
    "        x = torch.cat((x, nc_feat), dim=1)\n",
    "        x = self.final_fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3546564-e120-45f8-a4c9-9e4007ab6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Custom Gazelle Optimizer ---\n",
    "class GazelleOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, alpha=0.1, beta=0.5):\n",
    "        defaults = dict(lr=lr, alpha=alpha, beta=beta)\n",
    "        super(GazelleOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            alpha = group['alpha']\n",
    "            beta = group['beta']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                noise = torch.randn_like(grad) * beta\n",
    "                motion = torch.sign(grad + noise)\n",
    "                update = -lr * motion + alpha * torch.randn_like(p.data)\n",
    "\n",
    "                p.data.add_(update)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9f79032e-2fec-4468-a1a8-2ef410ca6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Training & Evaluation ---\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    optimizer = GazelleOptimizer(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        for images, nc_features, labels in train_loader:\n",
    "            images, labels, nc_features = images.to(device), labels.to(device), nc_features.to(device)\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, nc_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)\n",
    "            running_loss += loss.item()\n",
    "            preds = model(images, nc_features).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {running_loss:.4f} | Train Acc: {acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_val, total_val = 0, 0\n",
    "            for images, nc_features, labels in val_loader:\n",
    "                images, labels, nc_features = images.to(device), labels.to(device), nc_features.to(device)\n",
    "                outputs = model(images, nc_features)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "            val_acc = correct_val / total_val\n",
    "            print(f\"Validation Accuracy: {val_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c336a68d-c4a5-4477-872a-dcbd63972d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(os.listdir(data_dir))\n",
    "num_classes = len(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3c7c0db-043d-426b-a251-33c81f0ba900",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x544 and 39x7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SqueezeNetWithNC(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m----> 3\u001b[0m train_model(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[118], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m     21\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     22\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(images, nc_features)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 8\u001b[0m, in \u001b[0;36mGazelleOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m closure() \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m     10\u001b[0m         lr \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[118], line 15\u001b[0m, in \u001b[0;36mtrain_model.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images, nc_features)\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[115], line 23\u001b[0m, in \u001b[0;36mSqueezeNetWithNC.forward\u001b[1;34m(self, x, nc_ratio)\u001b[0m\n\u001b[0;32m     20\u001b[0m nc_feat \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc_fc(nc_ratio\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, nc_feat), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_fc(x)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x544 and 39x7)"
     ]
    }
   ],
   "source": [
    "model = SqueezeNetWithNC(num_classes=num_classes)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ec3b2-b74f-4893-b106-52b144a60dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b180c-6cb3-4764-89e1-6cee3b56a406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b9afdc2-588a-46aa-b4c8-6336695b2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Model -------------------------\n",
    "num_classes=7\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.base = models.squeezenet1_1(pretrained=True)\n",
    "        self.base.classifier = nn.Identity()\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, nc):\n",
    "        x = self.base.features(x)\n",
    "        x = self.gap(x).view(x.size(0), -1)\n",
    "        x = torch.cat((x, nc.unsqueeze(1)), dim=1)\n",
    "        out = self.fc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bbddc769-bb11-4199-b8c6-fb22c17ced47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Gazelle Optimizer (Replace with your real optimizer)\n",
    "class GazelleOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        defaults = dict(lr=lr)\n",
    "        super(GazelleOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.data -= group['lr'] * p.grad.data\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "264b1a31-ae28-435c-9880-d920772b234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation\n",
    "\n",
    "def train_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    train_accs, val_accs, train_losses, val_losses = [], [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "        for imgs, nc_ratios, labels in train_loader:\n",
    "            imgs, nc_ratios, labels = imgs.to(device), nc_ratios.to(device), labels.to(device)\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs, nc_ratios)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(imgs, nc_ratios).argmax(dim=1)\n",
    "                train_correct += (preds == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, nc_ratios, labels in val_loader:\n",
    "                imgs, nc_ratios, labels = imgs.to(device), nc_ratios.to(device), labels.to(device)\n",
    "                outputs = model(imgs, nc_ratios)\n",
    "                loss = criterion(outputs, labels)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% - Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    return train_accs, val_accs, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f229c942-f078-4275-8b94-f4a66c0e2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "def plot_metrics(train_accs, val_accs, train_losses, val_losses):\n",
    "    epochs = range(1, len(train_accs) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_accs, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accs, label='Val Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97557fa3-9c7b-4a39-b8c0-1ac86fe8b9bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CombinedModel.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(data_dir))\n\u001b[0;32m     20\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m CombinedModel(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GazelleOptimizer(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: CombinedModel.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "# Run Training\n",
    "num_classes=7\n",
    "data_dir = r\"C:\\Users\\CVR\\Desktop\\DataSet\\Herlev Dataset\\augmented_train\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = CervicalCancerDataset(data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "num_classes = len(os.listdir(data_dir))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CombinedModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = GazelleOptimizer(model.parameters(), lr=0.001)\n",
    "\n",
    "train_accs, val_accs, train_losses, val_losses = train_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10\n",
    ")\n",
    "\n",
    "plot_metrics(train_accs, val_accs, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7fd3b-0c00-497b-99b0-3ed2dfb1b5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eeb3fcf3-340f-4d97-96ee-5450240656e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Gazelle Optimizer -------------------------\n",
    "class GazelleOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super(GazelleOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                noise = torch.randn_like(grad)\n",
    "                levy = torch.randn_like(grad) * torch.sign(torch.randn_like(grad)) / (torch.abs(torch.randn_like(grad)) + 1e-6)\n",
    "                update = -group['lr'] * grad + 0.05 * noise + 0.01 * levy\n",
    "                p.data.add_(update)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fea5ca35-5f94-46c4-bfe6-76a12ba389b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Training -------------------------\n",
    "def train_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    train_accs, val_accs, train_losses, val_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "        for imgs, nc_ratios, labels in train_loader:\n",
    "            imgs, nc_ratios, labels = imgs.to(device), nc_ratios.to(device), labels.to(device)\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs, nc_ratios)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(imgs, nc_ratios).argmax(dim=1)\n",
    "                train_correct += (preds == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, nc_ratios, labels in val_loader:\n",
    "                imgs, nc_ratios, labels = imgs.to(device), nc_ratios.to(device), labels.to(device)\n",
    "                outputs = model(imgs, nc_ratios)\n",
    "                loss = criterion(outputs, labels)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% - Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    return train_accs, val_accs, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0b174c4-22d8-4a37-a68c-71f3ea3a1f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\CVR\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GazelleOptimizer(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     23\u001b[0m train_accs, val_accs, train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_validate(\n\u001b[1;32m---> 24\u001b[0m     model, loader, loader, criterion, optimizer, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------- Main -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = r\"C:\\Users\\CVR\\Desktop\\DataSet\\Herlev Dataset\\augmented_train\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    dataset = CervicalCancerDataset(data_dir, transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CombinedModel().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = GazelleOptimizer(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_accs, val_accs, train_losses, val_losses = train_validate(\n",
    "        model, loader, loader, criterion, optimizer, device, num_epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f3a9bf7-bf97-4e5f-804d-ccced9b34810",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1387400865.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[68], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.figure(figsize=(12, 5))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e900423-22d8-49c5-ad44-e344a2d66bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e1029-3832-4e53-b956-d0d70f824a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
